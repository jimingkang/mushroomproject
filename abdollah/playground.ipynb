{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/a/.local/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/a/.local/lib/python3.8/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/a/.local/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/a/.local/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/a/.local/lib/python3.8/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /home/a/.local/lib/python3.8/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/a/.local/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from matplotlib) (20.3)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/a/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rs_pipeline = rs.pipeline()\n",
    "rs_config = rs.config()\n",
    "rs_config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "rs_config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yolov8x.engine for TensorRT inference...\n",
      "[04/29/2024-02:57:22] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[04/29/2024-02:57:22] [TRT] [I] Loaded engine size: 275 MiB\n",
      "[04/29/2024-02:57:22] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +33, now: CPU 2132, GPU 23693 (MiB)\n",
      "[04/29/2024-02:57:22] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +1, GPU +274, now: CPU 1, GPU 423 (MiB)\n",
      "[04/29/2024-02:57:22] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +27, now: CPU 1857, GPU 23430 (MiB)\n",
      "[04/29/2024-02:57:22] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +330, now: CPU 1, GPU 753 (MiB)\n",
      "\n",
      "0: 736x1280 (no detections), 523.6ms\n",
      "Speed: 9.5ms preprocess, 523.6ms inference, 9.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 482.0ms\n",
      "Speed: 4.8ms preprocess, 482.0ms inference, 27.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 491.6ms\n",
      "Speed: 5.3ms preprocess, 491.6ms inference, 29.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 494.0ms\n",
      "Speed: 8.4ms preprocess, 494.0ms inference, 26.9ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 490.4ms\n",
      "Speed: 6.8ms preprocess, 490.4ms inference, 28.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 2 WBs, 482.6ms\n",
      "Speed: 7.7ms preprocess, 482.6ms inference, 23.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 485.6ms\n",
      "Speed: 7.1ms preprocess, 485.6ms inference, 23.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 483.3ms\n",
      "Speed: 6.5ms preprocess, 483.3ms inference, 27.3ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 4 WBs, 494.7ms\n",
      "Speed: 4.5ms preprocess, 494.7ms inference, 40.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 480.9ms\n",
      "Speed: 3.9ms preprocess, 480.9ms inference, 28.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 485.1ms\n",
      "Speed: 6.8ms preprocess, 485.1ms inference, 29.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 478.4ms\n",
      "Speed: 6.8ms preprocess, 478.4ms inference, 24.3ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 483.7ms\n",
      "Speed: 4.7ms preprocess, 483.7ms inference, 27.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 477.3ms\n",
      "Speed: 5.2ms preprocess, 477.3ms inference, 26.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 2 WBs, 475.8ms\n",
      "Speed: 8.5ms preprocess, 475.8ms inference, 33.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 482.9ms\n",
      "Speed: 4.1ms preprocess, 482.9ms inference, 30.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 475.1ms\n",
      "Speed: 9.9ms preprocess, 475.1ms inference, 23.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 2 WBs, 482.7ms\n",
      "Speed: 5.5ms preprocess, 482.7ms inference, 24.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 2 WBs, 488.0ms\n",
      "Speed: 4.3ms preprocess, 488.0ms inference, 33.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 522.2ms\n",
      "Speed: 6.0ms preprocess, 522.2ms inference, 36.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 1 BB, 4 WBs, 524.0ms\n",
      "Speed: 5.7ms preprocess, 524.0ms inference, 37.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 1 BB, 5 WBs, 514.3ms\n",
      "Speed: 8.5ms preprocess, 514.3ms inference, 33.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 1 BB, 3 WBs, 524.0ms\n",
      "Speed: 5.0ms preprocess, 524.0ms inference, 29.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 523.6ms\n",
      "Speed: 7.7ms preprocess, 523.6ms inference, 42.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 518.2ms\n",
      "Speed: 4.5ms preprocess, 518.2ms inference, 27.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 477.6ms\n",
      "Speed: 7.7ms preprocess, 477.6ms inference, 32.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 482.2ms\n",
      "Speed: 4.7ms preprocess, 482.2ms inference, 25.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 481.7ms\n",
      "Speed: 5.4ms preprocess, 481.7ms inference, 20.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 488.3ms\n",
      "Speed: 4.9ms preprocess, 488.3ms inference, 26.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 491.9ms\n",
      "Speed: 7.3ms preprocess, 491.9ms inference, 29.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 480.2ms\n",
      "Speed: 5.0ms preprocess, 480.2ms inference, 33.1ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 481.8ms\n",
      "Speed: 4.2ms preprocess, 481.8ms inference, 21.3ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 3 WBs, 482.6ms\n",
      "Speed: 5.1ms preprocess, 482.6ms inference, 21.7ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import ctypes\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8x.engine\", task=\"segment\")\n",
    "# Initialize RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a new frame\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert RealSense frame to OpenCV format\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        res = model.predict(color_image, imgsz=(736,1280))\n",
    "\n",
    "        # Perform object detection using YOLOv8\n",
    "        # Replace this with your actual object detection code using the YOLOv8 model\n",
    "        \n",
    "        # Display the results\n",
    "        cv2.imshow(\"Object Detection\", res[0].plot())\n",
    "        \n",
    "        # Check for the 'q' key to quit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                                2.1.0a0+41361538.nv23.6\n",
      "torchvision                          0.16.1                 \n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yolov8n.engine for TensorRT inference...\n",
      "[04/29/2024-02:54:05] [TRT] [I] Loaded engine size: 13 MiB\n",
      "[04/29/2024-02:54:06] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +616, GPU +870, now: CPU 1579, GPU 24717 (MiB)\n",
      "[04/29/2024-02:54:06] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +13, now: CPU 0, GPU 13 (MiB)\n",
      "[04/29/2024-02:54:06] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +28, now: CPU 1566, GPU 24718 (MiB)\n",
      "[04/29/2024-02:54:06] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +61, now: CPU 0, GPU 74 (MiB)\n",
      "\n",
      "0: 736x1280 1 WB, 39.7ms\n",
      "Speed: 7.6ms preprocess, 39.7ms inference, 443.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "Loading yolov8n.engine for TensorRT inference...\n",
      "[04/29/2024-02:54:08] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[04/29/2024-02:54:08] [TRT] [I] Loaded engine size: 13 MiB\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +32, now: CPU 1900, GPU 25517 (MiB)\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +13, now: CPU 0, GPU 87 (MiB)\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +26, now: CPU 1887, GPU 25492 (MiB)\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +62, now: CPU 0, GPU 149 (MiB)\n",
      "\n",
      "0: 736x1280 2 WBs, 40.5ms\n",
      "Speed: 7.9ms preprocess, 40.5ms inference, 7.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "Loading yolov8n.engine for TensorRT inference...\n",
      "[04/29/2024-02:54:08] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[04/29/2024-02:54:08] [TRT] [I] Loaded engine size: 13 MiB\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +32, now: CPU 1906, GPU 25670 (MiB)\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +1, GPU +13, now: CPU 1, GPU 162 (MiB)\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +25, now: CPU 1893, GPU 25670 (MiB)\n",
      "[04/29/2024-02:54:08] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +61, now: CPU 1, GPU 223 (MiB)\n",
      "\n",
      "0: 736x1280 6 WBs, 41.9ms\n",
      "Speed: 8.5ms preprocess, 41.9ms inference, 12.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "Loading yolov8n.engine for TensorRT inference...\n",
      "[04/29/2024-02:54:09] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[04/29/2024-02:54:09] [TRT] [I] Loaded engine size: 13 MiB\n",
      "[04/29/2024-02:54:09] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +33, now: CPU 1913, GPU 25926 (MiB)\n",
      "[04/29/2024-02:54:09] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +13, now: CPU 1, GPU 236 (MiB)\n",
      "[04/29/2024-02:54:09] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +26, now: CPU 1899, GPU 25928 (MiB)\n",
      "[04/29/2024-02:54:09] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +62, now: CPU 1, GPU 298 (MiB)\n",
      "\n",
      "0: 736x1280 7 WBs, 44.8ms\n",
      "Speed: 4.3ms preprocess, 44.8ms inference, 7.0ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 45.0ms\n",
      "Speed: 3.7ms preprocess, 45.0ms inference, 12.4ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 44.5ms\n",
      "Speed: 4.1ms preprocess, 44.5ms inference, 6.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 43.4ms\n",
      "Speed: 7.2ms preprocess, 43.4ms inference, 14.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 41.5ms\n",
      "Speed: 4.6ms preprocess, 41.5ms inference, 15.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 43.8ms\n",
      "Speed: 4.4ms preprocess, 43.8ms inference, 16.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 45.5ms\n",
      "Speed: 3.8ms preprocess, 45.5ms inference, 16.6ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 45.4ms\n",
      "Speed: 6.5ms preprocess, 45.4ms inference, 13.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 41.2ms\n",
      "Speed: 8.0ms preprocess, 41.2ms inference, 17.8ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 8 WBs, 46.5ms\n",
      "Speed: 4.7ms preprocess, 46.5ms inference, 15.5ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 8 WBs, 40.7ms\n",
      "Speed: 6.6ms preprocess, 40.7ms inference, 13.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 42.3ms\n",
      "Speed: 6.2ms preprocess, 42.3ms inference, 17.2ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "\n",
      "0: 736x1280 7 WBs, 45.3ms\n",
      "Speed: 4.3ms preprocess, 45.3ms inference, 16.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Frame didn't arrive within 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m worker_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Wait for a new frame\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     color_frame \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mget_color_frame()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m color_frame:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Frame didn't arrive within 5000"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "import ctypes\n",
    "from ultralytics import YOLO\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import cycle\n",
    "\n",
    "# Initialize queues\n",
    "input_queue = queue.Queue()\n",
    "output_queue = queue.Queue()\n",
    "\n",
    "# Load YOLOv8 models in separate instances\n",
    "def load_model():\n",
    "    return YOLO(\"yolov8n.engine\", task=\"segment\")\n",
    "\n",
    "# Number of model instances\n",
    "num_models = 4\n",
    "\n",
    "# Create a pool of YOLO models\n",
    "model_pool = [load_model() for _ in range(num_models)]\n",
    "\n",
    "# Prediction function to be run by the workers\n",
    "def process_image(data):\n",
    "    color_image, model = data\n",
    "    result = model.predict(color_image, imgsz=(736, 1280))\n",
    "    return result[0].plot()  # Assuming result[0].plot() returns the image with predictions drawn\n",
    "\n",
    "def worker(input_queue, output_queue, model_pool):\n",
    "    with ThreadPoolExecutor(max_workers=len(model_pool)) as executor:\n",
    "        model_cycle = cycle(model_pool)  # Create a cycle of models\n",
    "\n",
    "        while True:\n",
    "            if input_queue.empty():\n",
    "                continue  # Wait until the queue has an item\n",
    "\n",
    "            color_image = input_queue.get()  # Fetch the next image to process\n",
    "            if color_image is None:\n",
    "                break  # Stop if None is sent as a signal to terminate\n",
    "\n",
    "            # Get the next model from the cycle\n",
    "            model = next(model_cycle)\n",
    "            \n",
    "            # Submit the task to the executor\n",
    "            future = executor.submit(process_image, (color_image, model))\n",
    "            \n",
    "            # Process the result as soon as it's ready\n",
    "            result = future.result()\n",
    "            output_queue.put(result)\n",
    "\n",
    "# Initialize RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "# Main execution flow in the try block\n",
    "try:\n",
    "    # Start the worker thread\n",
    "    from threading import Thread\n",
    "    worker_thread = Thread(target=worker, args=(input_queue, output_queue, model_pool))\n",
    "    worker_thread.start()\n",
    "\n",
    "    while True:\n",
    "        # Wait for a new frame\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert RealSense frame to OpenCV format and add to the input queue\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        input_queue.put(color_image)\n",
    "\n",
    "        # Display results if available in the output queue\n",
    "        if not output_queue.empty():\n",
    "            result = output_queue.get()  # Get the next processed image\n",
    "            cv2.imshow(\"Object Detection\", result)\n",
    "\n",
    "        # Check for the 'q' key to quit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    input_queue.put(None)\n",
    "    worker_thread.join()\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
