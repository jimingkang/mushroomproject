#!/usr/bin/env python3
# -*- coding:utf-8 -*-
# Copyright (c) Megvii, Inc. and its affiliates.

# ROS2 rclpy -- Ar-Ray-code 2021

import os
import time
from loguru import logger

import cv2
from numpy import empty

import torch
import torch.backends.cudnn as cudnn

from yolox.data.data_augment import ValTransform
from yolox.data.datasets import COCO_CLASSES
from yolox.exp import get_exp
from yolox.utils import fuse_model, get_model_info, postprocess, setup_logger, vis

from .yolox_ros_py_utils.utils import yolox_py

import rclpy
from rclpy.node import Node

from std_msgs.msg import Header
from cv_bridge import CvBridge
from sensor_msgs.msg import Image

from rclpy.qos import qos_profile_sensor_data

from bboxes_ex_msgs.msg import BoundingBoxes
from bboxes_ex_msgs.msg import BoundingBox

# from darknet_ros_msgs.msg import BoundingBoxes
# from darknet_ros_msgs.msg import BoundingBox

class Predictor(object):
    def __init__(self, model, exp, cls_names=COCO_CLASSES, trt_file=None, decoder=None, device="gpu", fp16=True, legacy=False):
        self.model = model
        self.cls_names = cls_names
        self.decoder = decoder
        self.num_classes = exp.num_classes
        self.confthre = exp.test_conf
        self.threshold = exp.threshold
        self.test_size = exp.test_size
        self.device = device
        self.fp16 = fp16
        self.preproc = ValTransform(legacy=legacy)
        logger.info("self.num_classes:{},self.confthre:{},self.fp16:{}".format(self.num_classes,self.confthre,self.fp16))
        if trt_file is not None:
            from torch2trt import TRTModule

            model_trt = TRTModule()
            model_trt.load_state_dict(torch.load(trt_file))

            x = torch.ones(1, 3, exp.test_size[0], exp.test_size[1]).cuda()
            self.model(x)
            self.model = model_trt

    def inference(self, img):
        img_info = {"id": 0}
        if isinstance(img, str):
            img_info["file_name"] = os.path.basename(img)
            img = cv2.imread(img)
        else:
            img_info["file_name"] = None

        height, width = img.shape[:2]
        img_info["height"] = height
        img_info["width"] = width
        img_info["raw_img"] = img

        ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])
        img_info["ratio"] = ratio

        img, _ = self.preproc(img, None, self.test_size)
        img = torch.from_numpy(img).unsqueeze(0)
        img = img.float()
        if self.device == "gpu":
            img = img.cuda()
            if self.fp16:
                img = img.half()  # to FP16

        with torch.no_grad():
            t0 = time.time()
            outputs = self.model(img)
            if self.decoder is not None:
                outputs = self.decoder(outputs, dtype=outputs.type())
            outputs = postprocess(
                outputs, self.num_classes, self.confthre,
                self.threshold, class_agnostic=True
            )
            fps = int(1/(time.time() - t0))
            logger.info("{}fps".format(fps))
        return outputs, img_info

    def visual(self, output, img_info, cls_conf=0.35):
        ratio = img_info["ratio"]
        img = img_info["raw_img"]
        if output is None:
            return img
        output = output.cpu()

        bboxes = output[:, 0:4]

        # preprocessing: resize
        bboxes /= ratio

        cls = output[:, 6]
        scores = output[:, 4] * output[:, 5]

        vis_res = vis(img, bboxes, scores, cls, cls_conf, self.cls_names)
        
        return vis_res, bboxes, scores, cls, self.cls_names

class yolox_ros(yolox_py):
    def __init__(self) -> None:

        # ROS2 init
        super().__init__('yolox_ros', load_params=False)

        self.setting_yolox_exp()
        
        self.bridge = CvBridge()
        
        self.pub = self.create_publisher(BoundingBoxes,"bounding_boxes", 10)
        
        if (self.sensor_qos_mode):
            self.sub = self.create_subscription(Image,"image_raw",self.imageflow_callback, qos_profile_sensor_data)
        else:
            self.sub = self.create_subscription(Image,"image_raw",self.imageflow_callback, 10)

    def setting_yolox_exp(self) -> None:

        WEIGHTS_PATH = '../../weights/yolox_nano.pth'  #for no trt


        self.declare_parameter('imshow_isshow',True)
        #self.declare_parameter('yolox_exp_py', 'yolo_nano.py')
        self.declare_parameter('yolox_exp_py', 'yolox_vos_s.py')
        self.declare_parameter('fuse',False)
        self.declare_parameter('trt', True)
        self.declare_parameter('fp16', False)
        self.declare_parameter('legacy', False)
        self.declare_parameter('device', "gpu")
        # self.declare_parameter('', 0)
        self.declare_parameter('ckpt', WEIGHTS_PATH)
        self.declare_parameter('conf', 0.3)

        # nmsthre -> threshold
        self.declare_parameter('threshold', 0.65)
        # --tsize -> resize
        self.declare_parameter('resize', 640)
        
        self.declare_parameter('sensor_qos_mode', False)

        # =============================================================
        self.imshow_isshow = self.get_parameter('imshow_isshow').value

        exp_py = self.get_parameter('yolox_exp_py').value

        fuse = self.get_parameter('fuse').value
        trt = True#self.get_parameter('trt').value
        fp16 = self.get_parameter('fp16').value
        device = self.get_parameter('device').value

        ckpt = self.get_parameter('ckpt').value
        conf = self.get_parameter('conf').value
        legacy = self.get_parameter('legacy').value
        threshold = self.get_parameter('threshold').value
        
        input_shape_w = self.get_parameter('resize').value
        input_shape_h = input_shape_w

        self.sensor_qos_mode = self.get_parameter('sensor_qos_mode').value

        # ==============================================================

        cudnn.benchmark = True
        exp = get_exp(exp_py, None)


        BASE_PATH = os.getcwd()
        #file_name = os.path.join(BASE_PATH, "../YOLOX-main/YOLOX_outputs/yolox_voc_s/")
        file_name = "/home/jimmy/Downloads/mushroomproject/YOLOX-main/YOLOX_outputs/yolox_voc_s/"#ros2_ws/src/YOLOX-ROS/weights/tensorrt/"#os.path.join(BASE_PATH, "/src/YOLOX-ROS/weights/tensorrt/") #
        # os.makedirs(file_name, exist_ok=True)

        exp.test_conf = conf # test conf
        exp.threshold = threshold # nms threshold
        exp.test_size = (input_shape_h, input_shape_w) # test size

        model = exp.get_model()
        logger.info("Model Summary: {}".format(get_model_info(model, exp.test_size)))

        if device == "gpu":
            model.cuda()
            #if fp16:
            #    model.half() 
        # torch.cuda.set_device()
        # model.cuda()
        model.eval()

        # about not trt
        if not trt:
            if ckpt is None:
                ckpt_file = os.path.join(file_name, "best_ckpt.pth")
            else:
                ckpt_file = ckpt
            logger.info("loading checkpoint")
            ckpt = torch.load(ckpt_file, map_location="cpu")
            # load the model state dict
            model.load_state_dict(ckpt["model"],strict=False)
            logger.info("loaded checkpoint done.")

        # about fuse
        if fuse:
            logger.info("\tFusing model...")
            model = fuse_model(model)

        # TensorRT
        logger.info("trt: {},file_name:{}".format(trt,file_name))
        if trt:
            assert not fuse, "TensorRT model is not support model fusing!"
            trt_file = os.path.join(file_name, "model_trt.pth") 
            logger.info("trt_file:{}".format(trt_file))
            assert os.path.exists(
                trt_file
            ), "TensorRT model is not found!\n Run python3 tools/trt.py first!"
            model.head.decode_in_inference = False
            decoder = model.head.decode_outputs
            logger.info("Using TensorRT to inference")
        else:
            trt_file = None
            decoder = None
       
        self.predictor = Predictor(model, exp, COCO_CLASSES, trt_file, decoder, device, fp16, legacy)

    def imageflow_callback(self,msg:Image) -> None:
        try:
            img_rgb = self.bridge.imgmsg_to_cv2(msg,"bgr8")
            outputs, img_info = self.predictor.inference(img_rgb)
            logger.info("outputs: {},".format(outputs))

            try:
                result_img_rgb, bboxes, scores, cls, cls_names = self.predictor.visual(outputs[0], img_info)
                logger.info("bboxes: {},cls_names:{}".format(bboxes,cls_names))
                bboxes_msg = self.yolox2bboxes_msgs(bboxes, scores, cls, cls_names, msg.header, img_rgb)

                self.pub.publish(bboxes_msg)

                if (self.imshow_isshow):
                    cv2.imshow("YOLOX",result_img_rgb)
                    cv2.waitKey(1)

            except Exception as e:
                if (self.imshow_isshow):
                    cv2.imshow("YOLOX",img_rgb)
                    cv2.waitKey(1)
        except Exception as e:
            logger.error(e)
            pass

def ros_main(args = None):
    rclpy.init(args=args)
    ros_class = yolox_ros()

    try:
        rclpy.spin(ros_class)
    except KeyboardInterrupt:
        pass
    finally:
        ros_class.destroy_node()
        rclpy.shutdown()
    
if __name__ == "__main__":
    ros_main()

# 当前帧的角点和上一帧的进行配准
    def match_points(self,frame):
        #logger.info("match_points")
        bfmatch = cv2.BFMatcher(cv2.NORM_HAMMING)
        matches = bfmatch.knnMatch(frame.curr_des, frame.last_des, k=2)
        match_kps, idx1, idx2 = [], [], []

        for m,n in matches:
            if m.distance < 0.75*n.distance:
                idx1.append(m.queryIdx)
                idx2.append(m.trainIdx)

                p1 = frame.curr_kps[m.queryIdx]
                p2 = frame.last_kps[m.trainIdx]
                match_kps.append((p1, p2))
        assert len(match_kps) >= 8

        frame.curr_kps = frame.curr_kps[idx1]
        frame.last_kps = frame.last_kps[idx2]

        return match_kps
    def normalize(self,K, pts):
        Kinv = np.linalg.inv(K)
        # turn [[x,y]] -> [[x,y,1]]
        add_ones = lambda x: np.concatenate([x, np.ones((x.shape[0], 1))], axis=1)
        norm_pts = np.dot(Kinv, add_ones(pts).T).T[:, 0:2]
        return norm_pts
# 八点法对本质矩阵求解
    def fit_essential_matrix(self,match_kps):
        global K,frame
        match_kps = np.array(match_kps)

        # 使用相机内参对角点坐标归一化
        norm_curr_kps = self.normalize(K, match_kps[:, 0])
        norm_last_kps = self.normalize(K, match_kps[:, 1])

        # 求解本质矩阵和内点数据
        model, inliers = ransac((norm_last_kps, norm_curr_kps),
                                self.EssentialMatrixTransform,
                                min_samples=8,              # 最少需要 8 个点
                                residual_threshold=0.005,
                                max_trials=200)

        frame.curr_kps = frame.curr_kps[inliers]
        frame.last_kps = frame.last_kps[inliers]

        return model.params

    # 从本质矩阵中分解出相机运动 R、t
    def extract_Rt(E):
        W = np.mat([[0,-1,0],[1,0,0],[0,0,1]],dtype=float)
        U,d,Vt = np.linalg.svd(E)

        if np.linalg.det(U)  < 0: U  *= -1.0
        if np.linalg.det(Vt) < 0: Vt *= -1.0

        # 相机没有转弯，因此 R 的对角矩阵非常接近 diag([1,1,1])
        R = (np.dot(np.dot(U, W), Vt))
        if np.sum(R.diagonal()) < 0:
            R = np.dot(np.dot(U, W.T), Vt)

        t = U[:, 2]     # 相机一直向前，分量 t[2] > 0
        if t[2] < 0:
            t *= -1

        Rt = np.eye(4)
        Rt[:3, :3] = R
        Rt[:3, 3] = t
        return Rt          # Rt 为从相机坐标系的位姿变换到世界坐标系的位姿

    # opencv 的三角测量函数
    # def triangulate(pts1, pts2, pose1, pose2):
        # pts1 = normalize(pts1)
        # pts2 = normalize(pts2)

        # pose1 = np.linalg.inv(pose1)
        # pose2 = np.linalg.inv(pose2)

        # points4d = cv2.triangulatePoints(pose1[:3], pose2[:3], pts1.T, pts2.T).T
        # points4d /= points4d[:, 3:]
        # return points4d


    # 自己写的的三角测量函数
    def triangulate(self,pts1, pts2, pose1, pose2):
        global K
        pose1 = np.linalg.inv(pose1)            # 从世界坐标系变换到相机坐标系的位姿, 因此取逆
        pose2 = np.linalg.inv(pose2)

        pts1 = self.normalize(K, pts1)                 # 使用相机内参对角点坐标归一化
        pts2 = self.normalize(K, pts2)

        points4d = np.zeros((pts1.shape[0], 4))
        for i, (kp1, kp2) in enumerate(zip(pts1, pts2)):
            A = np.zeros((4,4))
            A[0] = kp1[0] * pose1[2] - pose1[0]
            A[1] = kp1[1] * pose1[2] - pose1[1]
            A[2] = kp2[0] * pose2[2] - pose2[0]
            A[3] = kp2[1] * pose2[2] - pose2[1]
            _, _, vt = np.linalg.svd(A)         # 对 A 进行奇异值分解
            points4d[i] = vt[3]

        points4d /= points4d[:, 3:]            # 归一化变换成齐次坐标 [x, y, z, 1]
        return points4d

    # 画出角点的运动轨迹
    def draw_points(self,frame):
        for kp1, kp2 in zip(frame.curr_kps, frame.last_kps):
            u1, v1 = int(kp1[0]), int(kp1[1])
            u2, v2 = int(kp2[0]), int(kp2[1])
            cv2.circle(frame.image, (u1, v1), color=(0,0,255), radius=3)
            cv2.line(frame.image, (u1, v1), (u2, v2), color=(255,0,0))
        return None

    # 筛选角点
    def check_points(self,points4d):
        # 判断3D点是否在两个摄像头前方
        good_points = points4d[:, 2] > 0
        # TODO: parallax、重投投影误差筛选等等 ....
        return good_points

    def process_frame(self,frame):
        global mapp
        # 提取当前帧的角点和描述子特征
        logger.info("---------------- process_frame----------------")
        frame.curr_kps, frame.curr_des = self.extract_points(frame)
        # 将角点位置和描述子通过类的属性传递给下一帧作为上一帧的角点信息
        Frame.last_kps, Frame.last_des = frame.curr_kps, frame.curr_des

        if frame.idx == 1:
            # 设置第一帧为初始帧，并以相机坐标系为世界坐标系
            frame.curr_pose = np.eye(4)
            points4d = [[0,0,0,1]]      # 原点为 [0, 0, 0] , 1 表示颜色
        else:
            # 角点配准, 此时会用 RANSAC 过滤掉一些噪声
            logger.info("---------------- match_points----------------")
            match_kps = self.match_points(frame)
            logger.info("frame: {}, curr_des: {}, last_des: {}, match_kps: {}".format(frame.idx, len(frame.curr_des), len(frame.last_des), len(match_kps)))
            # 使用八点法拟合出本质矩阵
            essential_matrix = self.fit_essential_matrix(match_kps)
            logger.info("---------------- Essential Matrix ----------------")
            logger.info(essential_matrix)
            # 利用本质矩阵分解出相机的位姿 Rt
            Rt = self.extract_Rt(essential_matrix)
            # 计算出当前帧相对于初始帧的相机位姿
            frame.curr_pose = np.dot(Rt, frame.last_pose)
            # 三角测量获得角点的深度信息
            points4d = self.triangulate(frame.last_kps, frame.curr_kps, frame.last_pose, frame.curr_pose)

            good_pt4d = self.check_points(points4d)
            points4d = points4d[good_pt4d]
            # TODO: g2o 后端优化
            self.draw_points(frame)
        mapp.add_observation(frame.curr_pose, points4d)     # 将当前的 pose 和点云放入地图中
        # 将当前帧的 pose 信息存储为下一帧的 last_pose 信息
        Frame.last_pose = frame.curr_pose
        return frame
    
    def extract_points(self,frame):
        logger.info("extract_points")

        orb = cv2.ORB_create()
        image = cv2.cvtColor(frame.image, cv2.COLOR_BGR2GRAY)
        # detection corners
        pts = cv2.goodFeaturesToTrack(image, 3000, qualityLevel=0.01, minDistance=3)
        #logger.info("pts: {}".format(pts ))
        # extract features
        kps = [cv2.KeyPoint(x=pt[0][0], y=pt[0][1], size=20) for pt in pts]
        logger.info("pts: {}".format(pts ))
        kps, des = orb.compute(image, kps)
        #logger.info("kps, des: {},{}".format(kps, des ))

        kps = np.array([(kp.pt[0], kp.pt[1]) for kp in kps])
        return kps, des
    def my_pose_estimation(self,depth, rgb,box):
        global i,pointsxyzrgb_total
        logger.info("before convertion depth:{}".format(depth.shape))
        intrinsics = self.intrinsics
        depth = np.asanyarray(depth)  # * depth_scale # 1000 mm => 0.001 meters
        rgb = np.asanyarray(rgb)
        #rows,cols  = depth.shape
        rows=480#self.intrinsics.width 
        cols=848#self.intrinsics.height
        logger.info("depth:{}".format(depth.shape))
        i=i+1
        rotation_results=[]
        for box in bboxes_msg.bounding_boxes:
            if 1:# not r.hexists("detections",box.class_id):
                col, row = np.meshgrid(np.arange(cols), np.arange(rows), sparse=True)
                row = row.astype(float)
                col = col.astype(float)
                logger.info("r:{},c:{}".format(row.shape,col.shape))
                logger.info("box.xmin:{},box.xmax:{},box.ymin:{},box.ymax:{}".format(box.xmin,box.xmax,box.ymin,box.ymax))
                valid = (depth[box.ymin:box.ymax,box.xmin:box.xmax] > 0) #& (depth < clip_distance_max) #remove from the depth image all values above a given value (meters).
                valid = np.ravel(valid)
                z = depth[box.ymin:box.ymax,box.xmin:box.xmax]
                #z = depth[box.xmin:box.xmax,box.ymin:box.ymax] 
                logger.info("z:{}".format(z.shape))
                x =  z * (col[:,box.xmin:box.xmax] - intrinsics.ppx) / intrinsics.fx
                y =  z * (row[box.ymin:box.ymax,:] - intrinsics.ppy) / intrinsics.fy
                #logger.info("x,y:{},{}".format(x,y))
                z = np.ravel(z)[valid]
                x = np.ravel(x)[valid]
                y = np.ravel(y)[valid]

                # Stack x, y, z coordinates into a point array
                points = np.vstack((x, y, z)).T

                # Reshape the RGB image for color mapping and normalize colors
                #red = np.ravel(rgb[box.ymin:box.ymax,box.xmin:box.xmax,0])[valid]
                #green = np.ravel(rgb[box.ymin:box.ymax,box.xmin:box.xmax,1])[valid]
                #blue = np.ravel(rgb[box.ymin:box.ymax,box.xmin:box.xmax,2])[valid]
                #colors= np.dstack((red, green, blue))
                #pointsxyzrgb = pointsxyzrgb.reshape(-1,6)
                #colors = colors.reshape(-1, 3)
                #colors = (colors).astype(np.uint8)  # Ensure colors are uint8 for PyVista

                # Camera intrinsic parameters (adjust if known)
                #fx = fy = 382  # Focal length in pixels
                #cx = rows / 2.0
                #cy = cols / 2.0

                # Convert pixel coordinates to 3D coordinates in camera space
                #X = (x - cx) * z / fx
                #Y = (y - cy) * z / fy
                #Z = z

                # Stack into Nx3 array
                #points_3d = np.vstack((X, Y, Z)).T
                x_min = box.xmin#int(x_min)
                y_min = box.ymin# int(y_min)
                #width_bbox = int(width_bbox)
                #height_bbox = int(height_bbox)
                x_max = box.xmax# + width_bbox
                y_max =box.ymax # + height_bbox

                # Handle cases where the bounding box is out of image bounds
                if x_min < 0 or y_min < 0 or x_max > rows or y_max > cols:
                    print("Bounding box out of image bounds for box ID:", box.class_id)
                    continue

                # Get indices within the bounding box
                bbox_indices = ((x >= x_min) & (x < x_max) & (y >= y_min) & (y < y_max))

                # Get the points and colors within the bounding box
                #X_bbox = X[bbox_indices]
                #Y_bbox = Y[bbox_indices]
                #Z_bbox = Z[bbox_indices]
                points_bbox = points#np.vstack((X_bbox, Y_bbox, Z_bbox)).T
                #logger.info("in pose  points_bbox:{}".format(points_bbox))

                if points_bbox.shape[0] < 3:
                    logger.info("Not enough valid points for plane fitting for annotation ID:{}".format(box.class_id))
                    continue

                # Fit a plane to the 3D points
                # Compute the centroid
                centroid = np.mean(points_bbox, axis=0)
                # Center the points
                points_centered = points_bbox - centroid
                # Compute covariance matrix
                H_matrix = np.dot(points_centered.T, points_centered)
                # Singular Value Decomposition
                _, _, Vt = np.linalg.svd(H_matrix)
                # Normal vector is the last row of Vt
                normal_vector = Vt[-1, :]
                # Ensure normal vector points towards the camera (positive Z)
                if normal_vector[2] > 0:
                    normal_vector = -normal_vector

                # Compute rotation matrix
                z_axis = np.array([0, 0, 1])
                rotation_axis = np.cross(z_axis, normal_vector)
                rotation_axis_norm = np.linalg.norm(rotation_axis)
                if rotation_axis_norm < 1e-6:
                    # The normal vector is aligned with the z-axis
                    rotation_matrix = np.eye(3)
                else:
                    rotation_axis = rotation_axis / rotation_axis_norm
                    angle = np.arccos(np.clip(np.dot(z_axis, normal_vector), -1.0, 1.0))
                    # Rodrigues' rotation formula
                    K = np.array([[0, -rotation_axis[2], rotation_axis[1]],
                                    [rotation_axis[2], 0, -rotation_axis[0]],
                                    [-rotation_axis[1], rotation_axis[0], 0]])
                    rotation_matrix = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * np.dot(K, K)

                # Convert rotation matrix to Euler angles (XYZ convention)
                r = R.from_matrix(rotation_matrix)
                euler_angles = r.as_euler('xyz', degrees=True)

                # Store rotation results
                rotation_results.append({
                    'annotation_id': 1,
                    'rotation_x': euler_angles[0],
                    'rotation_y': euler_angles[1],
                    'rotation_z': euler_angles[2],
                    'centroid': centroid,
                    'normal_vector': normal_vector
                })
                logger.info("Rotation{}\n,rotation_matrix{}".format(euler_angles,rotation_matrix))
                #time.sleep(1)
                
                
                #red = np.ravel(rgb[box.ymin:box.ymax,box.xmin:box.xmax,0])[valid]
                #green = np.ravel(rgb[box.ymin:box.ymax,box.xmin:box.xmax,1])[valid]
                #blue = np.ravel(rgb[box.ymin:box.ymax,box.xmin:box.xmax,2])[valid]
                #pointsxyzrgb = np.dstack((x, y, z, red, green, blue))
                #pointsxyzrgb = pointsxyzrgb.reshape(-1,6)
                #pointsxyzrgb_total=np.concatenate((pointsxyzrgb_total,pointsxyzrgb))
                #logger.info("pointsxyzrgb_total :{}".format(pointsxyzrgb_total))
                #logger.info("pointsxyzrgb_total:{},shape{}".format(pointsxyzrgb_total,pointsxyzrgb_total.shape))
        #self.create_point_cloud_file2(pointsxyzrgb_total,"new_room_total{}.ply".format(i))
        #i=i-1
        #pointsxyzrgb_total=np.delete(pointsxyzrgb_total,0)
        #return pointsxyzrgb_total
    def create_point_cloud_file2(self,vertices, filename):
        ply_header = '''ply
    format ascii 1.0
    element vertex %(vert_num)d
    property float x
    property float y
    property float z
    property uchar red
    property uchar green
    property uchar blue
    end_header
    '''
        with open(filename, 'w') as f:
            f.write(ply_header %dict(vert_num=len(vertices)))
            np.savetxt(f,vertices,'%f %f %f %d %d %d')
    def pose_estimation(self,depth,raw_img,bounding_boxes):
        print(" pose_estimation bounding_boxes:", bounding_boxes)
        # Convert depth image to proper format if necessary
        if depth.dtype != np.float32:
            depth = depth.astype(np.float32)
        # Convert depth units if necessary (e.g., scaling)
        # depth = depth * depth_scale  # Uncomment and set depth_scale if needed
        # Extract annotations for the specific image
        #image_id = get_image_id_by_name(image_name, coco)
        #annotations = [ann for ann in coco['annotations'] if ann['image_id'] == image_id]

        # Generate the 3D point cloud for the entire image using your provided code
        (height, width, _) = raw_img.shape
        xx, yy = np.meshgrid(np.arange(0, width), np.arange(0, height))

        # Flatten the arrays to create a point cloud
        x = xx.flatten()
        y = yy.flatten()
        z = depth.flatten()

        # Remove invalid depth points
        valid = (z > 0) & (~np.isnan(z))
        x = x[valid]
        y = y[valid]
        z = z[valid]

        # Stack x, y, z coordinates into a point array
        points = np.vstack((x, y, z)).T

        # Reshape the RGB image for color mapping and normalize colors
        colors = raw_img.reshape(-1, 3)[valid]
        colors = (colors).astype(np.uint8)  # Ensure colors are uint8 for PyVista

        # Camera intrinsic parameters (adjust if known)
        fx = fy = 382  # Focal length in pixels
        cx = width / 2.0
        cy = height / 2.0

        # Convert pixel coordinates to 3D coordinates in camera space
        X = (x - cx) * z / fx
        Y = (y - cy) * z / fy
        Z = z

        # Stack into Nx3 array
        points_3d = np.vstack((X, Y, Z)).T

        # Create a PyVista point cloud
        #cloud = pv.PolyData(points_3d)
        #cloud["RGB"] = colors

        # Create a PyVista plotter
        #plotter = pv.Plotter()

        # Add the point cloud to the plotter and set up color mapping
        #plotter.add_points(cloud, scalars="RGB", rgb=True, point_size=1)

        # Set plot labels
        #plotter.set_background("white")
        #plotter.add_axes()

        # Prepare to store rotation values and normals
        rotation_results = []
    
        # Process each mushroom instance
        for box in bounding_boxes:
            # Extract bounding box
            #bbox = ann['bbox']  # Format: [x_min, y_min, width, height]
            #x_min, y_min, width_bbox, height_bbox = bbox
            x_min = box.xmin#int(x_min)
            y_min = box.ymin# int(y_min)
            #width_bbox = int(width_bbox)
            #height_bbox = int(height_bbox)
            x_max = box.xmax# + width_bbox
            y_max =box.ymax # + height_bbox

            # Handle cases where the bounding box is out of image bounds
            if x_min < 0 or y_min < 0 or x_max > width or y_max > height:
                print("Bounding box out of image bounds for box ID:", box.class_id)
                continue

            # Get indices within the bounding box
            bbox_indices = ((x >= x_min) & (x < x_max) & (y >= y_min) & (y < y_max))

            # Get the points and colors within the bounding box
            X_bbox = X[bbox_indices]
            Y_bbox = Y[bbox_indices]
            Z_bbox = Z[bbox_indices]
            points_bbox = np.vstack((X_bbox, Y_bbox, Z_bbox)).T

            if points_bbox.shape[0] < 3:
                print("Not enough valid points for plane fitting for annotation ID:", box.class_id)
                continue

            # Fit a plane to the 3D points
            # Compute the centroid
            centroid = np.mean(points_bbox, axis=0)
            # Center the points
            points_centered = points_bbox - centroid
            # Compute covariance matrix
            H_matrix = np.dot(points_centered.T, points_centered)
            # Singular Value Decomposition
            _, _, Vt = np.linalg.svd(H_matrix)
            # Normal vector is the last row of Vt
            normal_vector = Vt[-1, :]
            # Ensure normal vector points towards the camera (positive Z)
            if normal_vector[2] > 0:
                normal_vector = -normal_vector

            # Compute rotation matrix
            z_axis = np.array([0, 0, 1])
            rotation_axis = np.cross(z_axis, normal_vector)
            rotation_axis_norm = np.linalg.norm(rotation_axis)
            if rotation_axis_norm < 1e-6:
                # The normal vector is aligned with the z-axis
                rotation_matrix = np.eye(3)
            else:
                rotation_axis = rotation_axis / rotation_axis_norm
                angle = np.arccos(np.clip(np.dot(z_axis, normal_vector), -1.0, 1.0))
                # Rodrigues' rotation formula
                K = np.array([[0, -rotation_axis[2], rotation_axis[1]],
                                [rotation_axis[2], 0, -rotation_axis[0]],
                                [-rotation_axis[1], rotation_axis[0], 0]])
                rotation_matrix = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * np.dot(K, K)

            # Convert rotation matrix to Euler angles (XYZ convention)
            r = R.from_matrix(rotation_matrix)
            euler_angles = r.as_euler('xyz', degrees=True)

            # Store rotation results
            rotation_results.append({
                'annotation_id': 1,
                'rotation_x': euler_angles[0],
                'rotation_y': euler_angles[1],
                'rotation_z': euler_angles[2],
                'centroid': centroid,
                'normal_vector': normal_vector
            })
            print(f"Rotation: {euler_angles}")

        # Show the plot
        #plotter.show()